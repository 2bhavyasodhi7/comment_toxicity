# -*- coding: utf-8 -*-
"""Toxic_Comment_Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F_Yru99zZ6KydGgrTV6p5t6RS73MHzf7
"""

!pip install gradio

import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader
from collections import defaultdict
from nltk.corpus import stopwords
from nltk.stem.snowball import SnowballStemmer
import gradio as gr
import nltk

nltk.download('stopwords')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

BATCH_SIZE = 64
EMBEDDING_DIM = 256
HIDDEN_DIM = 512
DROPOUT = 0.3
LEARNING_RATE = 0.001
EPOCHS = 10
MAX_LEN = 200

df = pd.read_csv('/content/jigsaw-toxic-comment-train.csv')
df = df.drop(columns=['id'], axis=1)

stemmer = SnowballStemmer('english')
stop_words = set(stopwords.words('english'))

def clean_text(text):
    text = text.lower()
    text = re.sub(r"what's", "what is ", text)
    text = re.sub(r"\'s", " ", text)
    text = re.sub(r"\'ve", " have ", text)
    text = re.sub(r"can't", "can not ", text)
    text = re.sub(r"n't", " not ", text)
    text = re.sub(r"i'm", "i am ", text)
    text = re.sub(r"\'re", " are ", text)
    text = re.sub(r"\'d", " would ", text)
    text = re.sub(r"\'ll", " will ", text)
    text = re.sub(r"\'scuse", " excuse ", text)
    text = re.sub(r"\W", " ", text)
    text = re.sub(r"\s+", " ", text)
    text = text.strip()
    return text

def preprocess_text(text):
    text = clean_text(text)
    tokens = [stemmer.stem(word) for word in text.split() if word not in stop_words]
    return " ".join(tokens)

df['processed_text'] = df['comment_text'].apply(preprocess_text)

labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']
X = df['processed_text'].values
y = df[labels].values

word_counts = defaultdict(int)
for text in X:
    for word in text.split():
        word_counts[word] += 1

vocab = {'<pad>': 0, '<unk>': 1}
for word, count in word_counts.items():
    if count >= 5:  # Only keep words appearing at least 5 times
        vocab[word] = len(vocab)

class ToxicDataset(Dataset):
    def __init__(self, texts, labels, vocab, max_len):
        self.texts = texts
        self.labels = labels
        self.vocab = vocab
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        tokens = text.split()[:self.max_len]
        indices = [self.vocab.get(token, 1) for token in tokens]  # 1 for <unk>

        # Padding
        if len(indices) < self.max_len:
            indices += [0] * (self.max_len - len(indices))
        else:
            indices = indices[:self.max_len]

        return {
            'text': torch.tensor(indices, dtype=torch.long),
            'label': torch.tensor(self.labels[idx], dtype=torch.float)
        }

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

train_dataset = ToxicDataset(X_train, y_train, vocab, MAX_LEN)
test_dataset = ToxicDataset(X_test, y_test, vocab, MAX_LEN)

def collate_fn(batch):
    texts = torch.stack([item['text'] for item in batch])
    labels = torch.stack([item['label'] for item in batch])
    return {'text': texts, 'label': labels}

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)

class ToxicLSTM(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim,
                           bidirectional=True, num_layers=2,
                           dropout=dropout, batch_first=True)
        self.fc = nn.Linear(hidden_dim*2, output_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, text):
        embedded = self.dropout(self.embedding(text))
        output, (hidden, cell) = self.lstm(embedded)
        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))
        return self.fc(hidden)

model = ToxicLSTM(
    vocab_size=len(vocab),
    embedding_dim=EMBEDDING_DIM,
    hidden_dim=HIDDEN_DIM,
    output_dim=6,  # 6 labels
    dropout=DROPOUT
).to(device)

def train(model, iterator, optimizer, criterion):
    model.train()
    epoch_loss = 0
    for batch in iterator:
        texts = batch['text'].to(device)
        labels = batch['label'].to(device)

        optimizer.zero_grad()
        predictions = model(texts)
        loss = criterion(predictions, labels)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
    return epoch_loss / len(iterator)

def evaluate(model, iterator, criterion):
    model.eval()
    epoch_loss = 0
    with torch.no_grad():
        for batch in iterator:
            texts = batch['text'].to(device)
            labels = batch['label'].to(device)
            predictions = model(texts)
            loss = criterion(predictions, labels)
            epoch_loss += loss.item()
    return epoch_loss / len(iterator)

criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

best_valid_loss = float('inf')
for epoch in range(EPOCHS):
    train_loss = train(model, train_loader, optimizer, criterion)
    valid_loss = evaluate(model, test_loader, criterion)

    if valid_loss < best_valid_loss:
        best_valid_loss = valid_loss
        torch.save(model.state_dict(), 'best-model.pt')

    print(f'Epoch: {epoch+1:02}')
    print(f'\tTrain Loss: {train_loss:.3f}')
    print(f'\tVal. Loss: {valid_loss:.3f}')

def predict_toxicity(text):
    # Preprocess input
    text = preprocess_text(text)
    tokens = text.split()[:MAX_LEN]
    indices = [vocab.get(token, 1) for token in tokens]

    # Pad sequence
    if len(indices) < MAX_LEN:
        indices += [0] * (MAX_LEN - len(indices))
    else:
        indices = indices[:MAX_LEN]

    # Convert to tensor
    tensor = torch.LongTensor(indices).unsqueeze(0).to(device)

    # Predict
    model.eval()
    with torch.no_grad():
        prediction = torch.sigmoid(model(tensor))

    # Format output
    return {label: float(prediction[0][i]) for i, label in enumerate(labels)}

# Create Gradio interface
interface = gr.Interface(
    fn=predict_toxicity,
    inputs=gr.Textbox(label="Enter Comment", lines=3),
    outputs=gr.Label(label="Toxicity Probabilities"),
    examples=[
        ["You're a stupid idiot and should die!"],
        ["I respectfully disagree with your opinion."],
        ["Your mother was a hamster and your father smelled of elderberries!"]
    ],
    title="Toxic Comment Classifier (LSTM)",
    description="RNN-based model to detect toxic comments with multi-label classification"
)

interface.launch()

def calculate_accuracy():
    model.eval()
    total_correct = 0
    total_samples = 0

    print("Starting accuracy calculation...")

    with torch.no_grad():
        for i, batch in enumerate(test_loader):
            texts = batch['text'].to(device)
            labels = batch['label'].to(device)

            predictions = model(texts)
            preds = (torch.sigmoid(predictions) > 0.5).float()  # FIXED LINE

            correct = (preds == labels).sum().item()
            total_correct += correct
            total_samples += labels.numel()

            if i % 100 == 0:
                print(f"Processed {i+1}/{len(test_loader)} batches")

    accuracy = total_correct / total_samples
    print(f"\nFinal Accuracy: {accuracy * 100:.2f}%")
    return accuracy

# MUST CALL THE FUNCTION TO SEE OUTPUT
print("\n=== Calculating Accuracy ===")
calculate_accuracy()